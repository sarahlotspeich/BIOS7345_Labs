---
title: "BIOS7345 Lab 5"
subtitle: "Testing coefficients"
author: "Sarah Lotspeich"
date: "5 October 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)

#Run this, then delete it
#install.packages("gmodels")

#Load packages
library(rms)
library(ggplot2)
library(magrittr)
library(gmodels)
library(dplyr)
```

#Introduction
A scientist named Dr. Bosenberg wanted to estimate the \underline{puncture distance for an epidural} in children by \underline{weight}. He regressed skin-to-epidural distance (`SED`) on weight (`WT`) and obtained a regression line. 

#Fitting models
```{r}
#read in the data
dat <- read.csv("https://raw.githubusercontent.com/sarahlotspeich/BIOS7345_Labs/master/Bios7345lab5.csv", 
                header=TRUE, 
                stringsAsFactors = FALSE)

#fit the model for SED ~ WT

```

Create a plot of this regression line over the data. Begin by writing a simple function to predict `SED` for a given `WT` based on your `mod` coefficients.

```{r}
#write function to predict SED from WT


#create a plot of the data


#overlay this plot with the regression line using stat_function()

```

He then simplified his regression equation for clinicals to a 1mm/kg rule such that $\hat{\beta}_1 = 1$ and $\hat{\beta}_0 = 0$. Overlay your plot with the simplified model: $\hat{\text{WT}} = 0 + 1 \times \hat{\text{SED}}.$

```{r}
#use geom_abline() to overlay x = y line


#print your plot

```

#Testing coefficients

Use the function `gmodels::estimable()` to obtain the estimates, SEs, and p-values for testing whether each regression coefficient equals 0 (i.e. those from the standard regression output). 

What contrast matrix, $\pmb{C}$, will give us $\pmb{\beta}_{1 \times 2}^T\pmb{C} = \pmb{0}_{1 \times 2}$? 
\begin{quote}
To test that $H_0: \beta_0 = 0$ and $H_0: \beta_1 = 0$, we need $\pmb{C} = \pmb{I}_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ so that $\pmb{\beta}_{1 \times 2}^T\pmb{C} = \pmb{\beta}_{1 \times 2}^T$. 
\end{quote}

Create the appropriate contrast matrix, `C`,

```{r}

```

and use it as the `cm` input for `gmodels::estimable()`. For `beta0` input a vector for the null values of the parameters.

```{r}

```

Compare this output to that from your model.

```{r}
#view model

```

Now, separately test the effect of `WT`, i.e. $$H_0: \beta_1 = 0 \text{ vs. } H_1: \beta_1 \neq 0.$$ What contrast matrix, $\pmb{C}$, will give us $\pmb{\beta}_{2 \times 1}\pmb{C} = \beta_1 = 0$? 
\begin{quote}
To test that $H_0: \beta_1 = 0$, we need $\pmb{C} = \begin{bmatrix} 0 & 1 \end{bmatrix}$ so that $\pmb{\beta}_{1 \times 2}^T\pmb{C} = 0\beta_0 + \beta_1 = \beta_1$. 
\end{quote}

Create the appropriate contrast matrix, `C`, and use `estimable()` to get the estimates, SEs, and p-values for this test.

```{r}

```

Obtain the joint test of whether $\begin{bmatrix}\beta_0 & \beta_1 \end{bmatrix}^T = \begin{bmatrix}0 & 1 \end{bmatrix}^T$. 

```{r}

```

Do you think Dr. Bosenberg's simplification of the original model was a good idea? 
\begin{quote}
No, we see from the test above that the slope and intercept from our model were significantly different from that in the simplified model.
\end{quote}

Now, code the joint test (for overall regression) by hand using matrices and the F-test. 

```{r}
#create design matrix/ response vector


#same contast matrix


#save some helpful constants


#Thrm 8.4a pg 199


#test statistic


#p-value

```

Does the F-test statistic match the test statistic obtained via the `estimable()` function? 

Note: the `estimable()` function returns a $\chi^2$ test statistic, but F and $\chi^2$ test statistics are really the same thing in that, after a normalization, $\chi^2$ is the limiting distribution of the F as the denominator degrees of freedom goes to infinity. The normalization is $$\chi^2 = \text{df}_{\text{num}}.F$$

```{r}
#use estimable()

#normalize the chi-squared test stat

```

Do the p-values match? Why or why not? 

```{r}
#get p-value

```

Now, hand code the test to jointly compare the fitted model to the simplification. 

```{r}
#vector of null values


#Thrm 8.4f/g, pg 203


#test statistic


#p-value

```

and compare this to the output from `estimable()`.

```{r}

```

What do we notice? 

`estimable()` uses `1-pchisq(F*2,2)`, i.e. a Wald Test (large sample). Recall from your notes that quadratic form $Q$ (Wald Test) is divided by degrees of freedom $q$ to derive $F$ statistic.